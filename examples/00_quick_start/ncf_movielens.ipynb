{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Collaborative Filtering on MovieLens dataset.\n",
    "\n",
    "Neural Collaborative Filtering (NCF) is a well known recommendation algorithm that generalizes the matrix factorization problem with multi-layer perceptron. \n",
    "\n",
    "This notebook provides an example of how to utilize and evaluate NCF implementation in the `recommenders`. We use a smaller dataset in this example to run NCF efficiently with GPU acceleration on a [Data Science Virtual Machine](https://azure.microsoft.com/en-gb/services/virtual-machines/data-science-virtual-machines/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adhi/miniconda3/envs/ncf/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.9.17 (main, Jul  5 2023, 20:41:20) \n",
      "[GCC 11.2.0]\n",
      "Pandas version: 1.5.3\n",
      "Tensorflow version: 2.7.4\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR') # only show error messages\n",
    "\n",
    "from recommenders.utils.timer import Timer\n",
    "from recommenders.models.ncf.ncf_singlenode import NCF\n",
    "from recommenders.models.ncf.dataset import Dataset as NCFDataset\n",
    "from recommenders.datasets import movielens\n",
    "from recommenders.utils.notebook_utils import is_jupyter\n",
    "from recommenders.datasets.python_splitters import python_chrono_split\n",
    "from recommenders.evaluation.python_evaluation import (rmse, mae, rsquared, exp_var, map_at_k, ndcg_at_k, precision_at_k, \n",
    "                                                     recall_at_k, get_top_k_items)\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Pandas version: {}\".format(pd.__version__))\n",
    "print(\"Tensorflow version: {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# top k items to recommend\n",
    "TOP_K = 10\n",
    "\n",
    "# Select MovieLens data size: 100k, 1m, 10m, or 20m\n",
    "MOVIELENS_DATA_SIZE = '1m'\n",
    "\n",
    "# Model parameters\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 16384\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Download the MovieLens dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:recommenders.datasets.download_utils:Downloading https://files.grouplens.org/datasets/movielens/ml-1m.zip\n",
      "100%|██████████| 5.78k/5.78k [00:07<00:00, 725KB/s]  \n"
     ]
    }
   ],
   "source": [
    "df = movielens.load_pandas_df(\n",
    "    size=MOVIELENS_DATA_SIZE,\n",
    "    header=[\"userID\", \"itemID\", \"rating\", \"timestamp\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split the data using the Spark chronological splitter provided in utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = python_chrono_split(df, 0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out any users or items in the test set that do not appear in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test[test[\"userID\"].isin(train[\"userID\"].unique())]\n",
    "test = test[test[\"itemID\"].isin(train[\"itemID\"].unique())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write datasets to csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"./train.csv\"\n",
    "test_file = \"./test.csv\"\n",
    "train.to_csv(train_file, index=False)\n",
    "test.to_csv(test_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate an NCF dataset object from the data subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:recommenders.models.ncf.dataset:Indexing ./train.csv ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:recommenders.models.ncf.dataset:Indexing ./test.csv ...\n",
      "INFO:recommenders.models.ncf.dataset:Creating full leave-one-out test file ./test_full.csv ...\n",
      "100%|██████████| 6040/6040 [02:37<00:00, 38.38it/s]\n",
      "INFO:recommenders.models.ncf.dataset:Indexing ./test_full.csv ...\n"
     ]
    }
   ],
   "source": [
    "data = NCFDataset(train_file=train_file, test_file=test_file, seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the NCF model on the training data, and get the top-k recommendations for our testing data\n",
    "\n",
    "NCF accepts implicit feedback and generates prospensity of items to be recommended to users in the scale of 0 to 1. A recommended item list can then be generated based on the scores. Note that this quickstart notebook is using a smaller number of epochs to reduce time for training. As a consequence, the model performance will be slighlty deteriorated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adhi/miniconda3/envs/ncf/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n",
      "2023-08-25 07:11:09.864033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-25 07:11:09.864138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-25 07:11:09.864180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-25 07:11:09.864714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-25 07:11:09.864738: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1609] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-08-25 07:11:09.864781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-25 07:11:09.864798: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3429 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "model = NCF (\n",
    "    n_users=data.n_users, \n",
    "    n_items=data.n_items,\n",
    "    model_type=\"NeuMF\",\n",
    "    n_factors=4,\n",
    "    layer_sizes=[16,8,4],\n",
    "    n_epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    learning_rate=1e-3,\n",
    "    verbose=1,\n",
    "    seed=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:recommenders.models.ncf.ncf_singlenode:Epoch 1 [22.49s]: train_loss = 0.500678 \n",
      "INFO:recommenders.models.ncf.ncf_singlenode:Epoch 2 [23.38s]: train_loss = 0.364449 \n",
      "INFO:recommenders.models.ncf.ncf_singlenode:Epoch 3 [22.57s]: train_loss = 0.353336 \n",
      "INFO:recommenders.models.ncf.ncf_singlenode:Epoch 4 [19.80s]: train_loss = 0.347568 \n",
      "INFO:recommenders.models.ncf.ncf_singlenode:Epoch 5 [24.01s]: train_loss = 0.341378 \n",
      "INFO:recommenders.models.ncf.ncf_singlenode:Epoch 6 [23.04s]: train_loss = 0.334813 \n",
      "INFO:recommenders.models.ncf.ncf_singlenode:Epoch 7 [22.71s]: train_loss = 0.327861 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mwith\u001b[39;00m Timer() \u001b[39mas\u001b[39;00m train_time:\n\u001b[0;32m----> 2\u001b[0m     model\u001b[39m.\u001b[39;49mfit(data)\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTook \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m seconds for training.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(train_time))\n",
      "File \u001b[0;32m~/miniconda3/envs/ncf/lib/python3.9/site-packages/recommenders/models/ncf/ncf_singlenode.py:392\u001b[0m, in \u001b[0;36mNCF.fit\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    389\u001b[0m train_loss \u001b[39m=\u001b[39m []\n\u001b[1;32m    391\u001b[0m \u001b[39m# calculate loss and update NCF parameters\u001b[39;00m\n\u001b[0;32m--> 392\u001b[0m \u001b[39mfor\u001b[39;00m user_input, item_input, labels \u001b[39min\u001b[39;00m data\u001b[39m.\u001b[39mtrain_loader(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size):\n\u001b[1;32m    394\u001b[0m     user_input \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muser2id[x] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m user_input])\n\u001b[1;32m    395\u001b[0m     item_input \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitem2id[x] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m item_input])\n",
      "File \u001b[0;32m~/miniconda3/envs/ncf/lib/python3.9/site-packages/recommenders/models/ncf/dataset.py:517\u001b[0m, in \u001b[0;36mDataset.train_loader\u001b[0;34m(self, batch_size, shuffle_size, yield_id, write_to)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_datafile \u001b[39mas\u001b[39;00m train_datafile:\n\u001b[1;32m    516\u001b[0m     \u001b[39mfor\u001b[39;00m user \u001b[39min\u001b[39;00m train_datafile\u001b[39m.\u001b[39musers:\n\u001b[0;32m--> 517\u001b[0m         user_positive_examples \u001b[39m=\u001b[39m train_datafile\u001b[39m.\u001b[39;49mload_data(user)\n\u001b[1;32m    518\u001b[0m         user_positive_item_pool \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(\n\u001b[1;32m    519\u001b[0m             user_positive_examples[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcol_item]\u001b[39m.\u001b[39munique()\n\u001b[1;32m    520\u001b[0m         )\n\u001b[1;32m    521\u001b[0m         n_samples \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_neg \u001b[39m*\u001b[39m user_positive_examples\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/ncf/lib/python3.9/site-packages/recommenders/models/ncf/dataset.py:203\u001b[0m, in \u001b[0;36mDataFile.load_data\u001b[0;34m(self, key, by_user)\u001b[0m\n\u001b[1;32m    201\u001b[0m records\u001b[39m.\u001b[39mappend(row)\n\u001b[1;32m    202\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mend_of_file:\n\u001b[0;32m--> 203\u001b[0m     \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    204\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ncf/lib/python3.9/site-packages/recommenders/models/ncf/dataset.py:107\u001b[0m, in \u001b[0;36mDataFile.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    106\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m  \u001b[39m# end of file\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnext_row \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_extract_row_data(\u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreader, \u001b[39mNone\u001b[39;49;00m))\n\u001b[1;32m    108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mline_num \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    110\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrow\n",
      "File \u001b[0;32m~/miniconda3/envs/ncf/lib/python3.9/csv.py:111\u001b[0m, in \u001b[0;36mDictReader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mline_num \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    109\u001b[0m     \u001b[39m# Used only for its side effect.\u001b[39;00m\n\u001b[1;32m    110\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfieldnames\n\u001b[0;32m--> 111\u001b[0m row \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreader)\n\u001b[1;32m    112\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mline_num \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreader\u001b[39m.\u001b[39mline_num\n\u001b[1;32m    114\u001b[0m \u001b[39m# unlike the basic reader, we prefer not to return blanks,\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[39m# because we will typically wind up with a dict full of None\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[39m# values\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ncf/lib/python3.9/codecs.py:319\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_buffer_decode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, errors, final):\n\u001b[1;32m    315\u001b[0m     \u001b[39m# Overwrite this method in subclasses: It must decode input\u001b[39;00m\n\u001b[1;32m    316\u001b[0m     \u001b[39m# and return an (output, length consumed) tuple\u001b[39;00m\n\u001b[1;32m    317\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m\n\u001b[0;32m--> 319\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    320\u001b[0m     \u001b[39m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer \u001b[39m+\u001b[39m \u001b[39minput\u001b[39m\n\u001b[1;32m    322\u001b[0m     (result, consumed) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer_decode(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39merrors, final)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with Timer() as train_time:\n",
    "    model.fit(data)\n",
    "\n",
    "print(\"Took {} seconds for training.\".format(train_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the movie recommendation use case scenario, seen movies are not recommended to the users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 2.7835 seconds for prediction.\n"
     ]
    }
   ],
   "source": [
    "with Timer() as test_time:\n",
    "    users, items, preds = [], [], []\n",
    "    item = list(train.itemID.unique())\n",
    "    for user in train.userID.unique():\n",
    "        user = [user] * len(item) \n",
    "        users.extend(user)\n",
    "        items.extend(item)\n",
    "        preds.extend(list(model.predict(user, item, is_list=True)))\n",
    "\n",
    "    all_predictions = pd.DataFrame(data={\"userID\": users, \"itemID\":items, \"prediction\":preds})\n",
    "\n",
    "    merged = pd.merge(train, all_predictions, on=[\"userID\", \"itemID\"], how=\"outer\")\n",
    "    all_predictions = merged[merged.rating.isnull()].drop('rating', axis=1)\n",
    "\n",
    "print(\"Took {} seconds for prediction.\".format(test_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluate how well NCF performs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ranking metrics are used for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP:\t0.049650\n",
      "NDCG:\t0.200524\n",
      "Precision@K:\t0.183033\n",
      "Recall@K:\t0.102721\n"
     ]
    }
   ],
   "source": [
    "eval_map = map_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\n",
    "eval_ndcg = ndcg_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\n",
    "eval_precision = precision_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\n",
    "eval_recall = recall_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\n",
    "\n",
    "print(\"MAP:\\t%f\" % eval_map,\n",
    "      \"NDCG:\\t%f\" % eval_ndcg,\n",
    "      \"Precision@K:\\t%f\" % eval_precision,\n",
    "      \"Recall@K:\\t%f\" % eval_recall, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_jupyter():\n",
    "    # Record results with papermill for tests\n",
    "    import papermill as pm\n",
    "    import scrapbook as sb\n",
    "    sb.glue(\"map\", eval_map)\n",
    "    sb.glue(\"ndcg\", eval_ndcg)\n",
    "    sb.glue(\"precision\", eval_precision)\n",
    "    sb.glue(\"recall\", eval_recall)\n",
    "    sb.glue(\"train_time\", train_time.interval)\n",
    "    sb.glue(\"test_time\", test_time.interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "ncf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
